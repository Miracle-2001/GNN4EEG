{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Train and Predict \n",
    "In this section, we describe the models.GNNModel.train and models.GNNModel.predict functions usages via a simple example on a random generated dataset. All the four models, i.e. DGCNN, RGNN, SparseDGCNN, HetEmotionNet are in turn initialized and used. \n",
    "\n",
    "Note that the dataset is randomly generated, so the predicting accuracy is roughly equals to 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjtuzky/.conda/envs/FACED/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.708246 10.042033]\n",
      " [ 2.491506 10.115846]\n",
      " [-0.085834  6.512851]\n",
      " [-3.800103  6.358406]\n",
      " [ 3.697464  6.43073 ]\n",
      " [-6.248761  6.355425]\n",
      " [ 6.235231  6.503184]\n",
      " [-2.389717  4.129234]\n",
      " [ 2.215655  4.134819]\n",
      " [-5.93043   3.860679]\n",
      " [ 5.847392  3.913364]\n",
      " [-0.088036  1.769319]\n",
      " [-4.540056  1.579296]\n",
      " [ 4.410568  1.591498]\n",
      " [-7.135999  1.234258]\n",
      " [ 6.886154  1.261932]\n",
      " [-2.414903 -0.698447]\n",
      " [ 2.352306 -0.703254]\n",
      " [-5.86441  -0.960142]\n",
      " [ 5.852648 -0.967906]\n",
      " [-0.110991 -2.935592]\n",
      " [-3.734979 -3.000179]\n",
      " [ 3.632534 -3.002585]\n",
      " [-5.901261 -3.338525]\n",
      " [ 5.600342 -3.331862]\n",
      " [-2.709502 -4.725389]\n",
      " [ 2.428215 -4.739219]\n",
      " [-0.152567 -6.130292]\n",
      " [-2.394174 -6.109372]\n",
      " [ 2.093873 -6.087955]]\n"
     ]
    }
   ],
   "source": [
    "from ge.models import *\n",
    "\n",
    "# based hyper-parameters defination \n",
    "num_nodes=30\n",
    "num_hiddens=10\n",
    "num_layers=2\n",
    "\n",
    "# load electrode_position \n",
    "electrode_position=np.load('./src/utils&others/pos.npy')[:30]*100\n",
    "print(electrode_position)\n",
    "global_connections=[[0,1],[5,6],[7,8],[11,12],[13,14]]\n",
    "\n",
    "# model initialization\n",
    "model_DGCNN=DGCNN(num_nodes,num_hiddens,num_layers,electrode_position)\n",
    "model_RGNN=RGNN(num_nodes,num_hiddens,num_layers,electrode_position,global_connections)\n",
    "model_SparseDGCNN=SparseDGCNN(num_nodes,num_hiddens,num_layers,electrode_position)\n",
    "model_Het=HetEmotionNet(num_nodes,num_hiddens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset preparations\n",
    "train_samples=30\n",
    "test_samples=6\n",
    "time_step=200\n",
    "\n",
    "raw_data_train=np.random.uniform(0,100,(train_samples,num_nodes,time_step))\n",
    "label_train=np.random.randint(0,2,(train_samples,))\n",
    "\n",
    "raw_data_test=np.random.uniform(0,100,(test_samples,num_nodes,time_step))\n",
    "label_test=np.random.randint(0,2,(test_samples,))\n",
    "\n",
    "\n",
    "\n",
    "freqs = [[1,4],[4, 8], [8, 13], [13, 30], [30, 47]] # using 5 bands\n",
    "\n",
    "\n",
    "import mne\n",
    "def get_freq_data(data): # extract frequency DE features from time-domain raw data \n",
    "    de = np.zeros((data.shape[0], data.shape[1], len(freqs)))\n",
    "    for i in range(len(freqs)):\n",
    "        print('Current freq band: ', freqs[i])\n",
    "        for sub in range(data.shape[0]):\n",
    "            data_video = data[sub, :, :] #(num_nodes,time_step)\n",
    "            low_freq = freqs[i][0]\n",
    "            high_freq = freqs[i][1]\n",
    "            data_video_filt = mne.filter.filter_data(\n",
    "                data_video, time_step, l_freq=low_freq, h_freq=high_freq) \n",
    "            data_video_filt = data_video_filt.reshape(\n",
    "                num_nodes,time_step)  \n",
    "            de_one = 0.5*np.log(2*np.pi*np.exp(1) *\n",
    "                                (np.var(data_video_filt, 1)))  # (num_nodes,)\n",
    "            de[sub, :, i] = de_one\n",
    "    return de\n",
    "\n",
    "data_train=get_freq_data(raw_data_train) \n",
    "data_test=get_freq_data(raw_data_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30, 200)\n",
      "(30,)\n",
      "(6, 30, 200)\n",
      "(6,)\n",
      "(30, 30, 5)\n",
      "(6, 30, 5)\n"
     ]
    }
   ],
   "source": [
    "print(raw_data_train.shape) # (30,30,200)\n",
    "print(label_train.shape) # (30,)\n",
    "print(raw_data_test.shape) # (6,30,200)\n",
    "print(label_test.shape) # (6,)\n",
    "print(data_train.shape) # (30,30,5)\n",
    "print(data_test.shape) # (6,30,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DGCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in train_only num_fe None\n",
      "gpu device  cpu\n",
      "training epochs :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epochs :  1\n",
      "training epochs :  2\n",
      "training epochs :  3\n",
      "training epochs :  4\n",
      "training epochs :  5\n",
      "training epochs :  6\n",
      "training epochs :  7\n",
      "training epochs :  8\n",
      "training epochs :  9\n",
      "DGCNN train list:  [0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6]\n",
      "test accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# DGCNN \n",
    "# define the training hyper-parameters and start training\n",
    "train_acc_list=model_DGCNN.train(data_train,label_train,device=torch.device('cpu'),\n",
    "            optimizer=torch.optim.Adam,num_classes=2,dropout=0.5,\n",
    "            batch_size=32,lr=0.001,l1_reg=0,l2_reg=0,num_epoch=10)\n",
    "print('DGCNN train list: ',train_acc_list)\n",
    "# make predictions\n",
    "predictions=model_DGCNN.predict(data_test)\n",
    "cnt=np.sum(predictions==label_test)\n",
    "print('test accuracy: ',cnt/test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in train_only num_fe None\n",
      "gpu device  cpu\n",
      "training epochs :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epochs :  1\n",
      "training epochs :  2\n",
      "training epochs :  3\n",
      "training epochs :  4\n",
      "training epochs :  5\n",
      "training epochs :  6\n",
      "training epochs :  7\n",
      "training epochs :  8\n",
      "training epochs :  9\n",
      "RGNN train list: [0.4, 0.36666666666666664, 0.5666666666666667, 0.4, 0.6, 0.4, 0.43333333333333335, 0.4, 0.36666666666666664, 0.3333333333333333]\n",
      "test accuracy:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# RGNN \n",
    "# define the training hyper-parameters and start training\n",
    "train_acc_list=model_RGNN.train(data_train,label_train,valid_data=data_test,device=torch.device('cpu'),\n",
    "            optimizer=torch.optim.Adam,num_classes=2,dropout=0.5,NodeDAT=True,\n",
    "            batch_size=32,lr=0.001,l1_reg=0,l2_reg=0,num_epoch=10)\n",
    "print('RGNN train list:',train_acc_list)\n",
    "# make predictions\n",
    "predictions=model_RGNN.predict(data_test)\n",
    "cnt=np.sum(predictions==label_test)\n",
    "print('test accuracy: ',cnt/test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SparseDGCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in train_only num_fe None\n",
      "gpu device  cpu\n",
      "training epochs :  0\n",
      "training epochs :  1\n",
      "training epochs :  2\n",
      "training epochs :  3\n",
      "training epochs :  4\n",
      "training epochs :  5\n",
      "training epochs :  6\n",
      "training epochs :  7\n",
      "training epochs :  8\n",
      "training epochs :  9\n",
      "SparseDGCNN train list: [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.43333333333333335, 0.4666666666666667, 0.5666666666666667, 0.6333333333333333]\n",
      "test accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# SparseDGCNN \n",
    "# define the training hyper-parameters and start training\n",
    "train_acc_list=model_SparseDGCNN.train(data_train,label_train,device=torch.device('cpu'),\n",
    "            optimizer=torch.optim.Adam,num_classes=2,dropout=0.5,\n",
    "            batch_size=32,lr=0.001,l1_reg=0,l2_reg=0,num_epoch=10)\n",
    "print('SparseDGCNN train list:',train_acc_list)\n",
    "# make predictions\n",
    "predictions=model_SparseDGCNN.predict(data_test)\n",
    "cnt=np.sum(predictions==label_test)\n",
    "print('test accuracy: ',cnt/test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HetEmotionNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in train_only num_fe 5\n",
      "gpu device  cpu\n",
      "num_freq 5\n",
      "s1 (30, 30, 200)\n",
      "s2 (30, 30, 5)\n",
      "s3 torch.Size([30, 30, 30])\n",
      "training epochs :  0\n",
      "training epochs :  1\n",
      "training epochs :  2\n",
      "training epochs :  3\n",
      "training epochs :  4\n",
      "training epochs :  5\n",
      "training epochs :  6\n",
      "training epochs :  7\n",
      "training epochs :  8\n",
      "training epochs :  9\n",
      "HetEmotionNet train list: [0.36666666666666664, 0.6, 0.6, 0.5333333333333333, 0.43333333333333335, 0.4, 0.4666666666666667, 0.43333333333333335, 0.4666666666666667, 0.5666666666666667]\n",
      "test accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# HetEmotionNet \n",
    "# define the training hyper-parameters and start training\n",
    "train_acc_list=model_Het.train(data_train,raw_data_train,label_train,device=torch.device('cpu'),\n",
    "            optimizer=torch.optim.Adam,num_classes=2,dropout=0.5,\n",
    "            batch_size=32,lr=0.001,l1_reg=0,l2_reg=0,num_epoch=10)\n",
    "print('HetEmotionNet train list:',train_acc_list)\n",
    "# make predictions\n",
    "predictions=model_Het.predict(data_test,raw_data_test)\n",
    "cnt=np.sum(predictions==label_test)\n",
    "print('test accuracy: ',cnt/test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and Load\n",
    "Here, we show the usage of models.GNNModel.save and models.GNNModel.load functions. We use DGCNN model as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "model_DGCNN.save('./','my_DGCNN_model.dic.pkl')\n",
    "new_model_DGCNN=DGCNN(1,1,1,[[1,1]])\n",
    "new_model_DGCNN.load('./','my_DGCNN_model.dic.pkl')\n",
    "predictions=new_model_DGCNN.predict(data_test)\n",
    "cnt=np.sum(predictions==label_test)\n",
    "print('test accuracy: ',cnt/test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Make Evalutaions \n",
    "In this section, we show the usages of 'protocols.data_split', 'protocols.data_FACED' and 'protocols.evaluation' functions. Specifically, based on the model and dataset initialized above, we successively apply 'ncv', 'cv' and 'fcv' cross-validation protocols to figure out the best hyper-parameters and make evaluations. \n",
    "\n",
    "Note that the dataset is randomly generated, so the predicting accuracy is roughly equals to 0.5. And we mainly focus on the usage of data_split function and briefly show the usage of data_FACED at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36,)\n"
     ]
    }
   ],
   "source": [
    "from ge.protocols import *\n",
    "\n",
    "# make further definations,  combine data_train and data_test\n",
    "K=4\n",
    "inner_K=3\n",
    "data=np.concatenate((data_train,data_test),axis=0)\n",
    "label=np.concatenate((label_train,label_test),axis=0)\n",
    "raw_data=np.concatenate((raw_data_train,raw_data_test),axis=0)\n",
    "subject_num=10\n",
    "section_size=data.shape[0]/subject_num\n",
    "# generate the subject_id_list for each sample\n",
    "subject_id_list=np.array([int(i/section_size) for i in range(data.shape[0])])\n",
    "print(subject_id_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ncv\n",
    "We first present the evaluation methods under 'ncv' protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3055555555555556 [{'fold': 0, 'best_paras': {'lr': 0.01, 'hiddens': 10}, 'train_acc_mean': 0.6785714285714286, 'test_acc_mean': 0.375, 'test_num_samples': 8}, {'fold': 1, 'best_paras': {'lr': 0.001, 'hiddens': 10}, 'train_acc_mean': 0.6551724137931034, 'test_acc_mean': 0.42857142857142855, 'test_num_samples': 7}, {'fold': 2, 'best_paras': {'lr': 0.001, 'hiddens': 10}, 'train_acc_mean': 0.3793103448275862, 'test_acc_mean': 0.14285714285714285, 'test_num_samples': 7}, {'fold': 3, 'best_paras': {'lr': 0.01, 'hiddens': 20}, 'train_acc_mean': 0.5909090909090909, 'test_acc_mean': 0.2857142857142857, 'test_num_samples': 14}]\n"
     ]
    }
   ],
   "source": [
    "# DGCNN and 'ncv'\n",
    "loader=data_split('cross_subject',data,label,subject_id_list)\n",
    "mean_acc,out_acc_list=evaluation(model_DGCNN,loader,'ncv',grid={\"lr\":[0.01,0.001],\"hiddens\":[10,20],'epoch':list(range(0,10))},\n",
    "                  categories=2,K=K,K_inner=inner_K,device=torch.device('cpu'),optimizer='Adam')\n",
    "print(mean_acc,out_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444 [{'fold': 0, 'best_paras': {'lr': 0.01, 'hiddens': 10}, 'train_acc_mean': 0.7142857142857143, 'test_acc_mean': 0.375, 'test_num_samples': 8}, {'fold': 1, 'best_paras': {'lr': 0.01, 'hiddens': 10}, 'train_acc_mean': 0.6551724137931034, 'test_acc_mean': 0.42857142857142855, 'test_num_samples': 7}, {'fold': 2, 'best_paras': {'lr': 0.001, 'hiddens': 20}, 'train_acc_mean': 0.5517241379310345, 'test_acc_mean': 0.8571428571428571, 'test_num_samples': 7}, {'fold': 3, 'best_paras': {'lr': 0.01, 'hiddens': 20}, 'train_acc_mean': 0.6818181818181818, 'test_acc_mean': 0.2857142857142857, 'test_num_samples': 14}]\n"
     ]
    }
   ],
   "source": [
    "# SparseDGCNN and 'ncv'\n",
    "mean_acc,out_acc_list=evaluation(model_SparseDGCNN,loader,'ncv',grid={\"lr\":[0.01,0.001],\"hiddens\":[10,20],'epoch':list(range(0,10))},\n",
    "                  categories=2,K=K,K_inner=inner_K,device=torch.device('cpu'),optimizer='Adam')\n",
    "print(mean_acc,out_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444 [{'fold': 0, 'best_paras': {'lr': 0.01, 'hiddens': 10}, 'train_acc_mean': 0.7142857142857143, 'test_acc_mean': 0.375, 'test_num_samples': 8}, {'fold': 1, 'best_paras': {'lr': 0.01, 'hiddens': 10}, 'train_acc_mean': 0.7241379310344828, 'test_acc_mean': 0.42857142857142855, 'test_num_samples': 7}, {'fold': 2, 'best_paras': {'lr': 0.01, 'hiddens': 20}, 'train_acc_mean': 0.5862068965517241, 'test_acc_mean': 0.8571428571428571, 'test_num_samples': 7}, {'fold': 3, 'best_paras': {'lr': 0.01, 'hiddens': 10}, 'train_acc_mean': 0.5909090909090909, 'test_acc_mean': 0.2857142857142857, 'test_num_samples': 14}]\n"
     ]
    }
   ],
   "source": [
    "# RGNN and 'ncv'\n",
    "mean_acc,out_acc_list=evaluation(model_RGNN,loader,'ncv',grid={\"lr\":[0.01,0.001],\"hiddens\":[10,20],'epoch':list(range(0,10))},\n",
    "                  categories=2,K=K,K_inner=inner_K,device=torch.device('cpu'),optimizer='Adam',NodeDAT=True)\n",
    "print(mean_acc,out_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4722222222222222 [{'fold': 0, 'best_paras': {'lr': 0.01, 'hiddens': 10}, 'train_acc_mean': 0.6785714285714286, 'test_acc_mean': 0.375, 'test_num_samples': 8}, {'fold': 1, 'best_paras': {'lr': 0.01, 'hiddens': 10}, 'train_acc_mean': 0.6551724137931034, 'test_acc_mean': 0.42857142857142855, 'test_num_samples': 7}, {'fold': 2, 'best_paras': {'lr': 0.01, 'hiddens': 10}, 'train_acc_mean': 0.6551724137931034, 'test_acc_mean': 0.14285714285714285, 'test_num_samples': 7}, {'fold': 3, 'best_paras': {'lr': 0.01, 'hiddens': 10}, 'train_acc_mean': 0.6818181818181818, 'test_acc_mean': 0.7142857142857143, 'test_num_samples': 14}]\n"
     ]
    }
   ],
   "source": [
    "# HetEmotionNet and 'ncv'\n",
    "loader=data_split('cross_subject',data,label,subject_id_list,raw_data)\n",
    "mean_acc,out_acc_list=evaluation(model_Het,loader,'ncv',grid={\"lr\":[0.01,0.001],\"hiddens\":[10,20],'epoch':list(range(0,10))},\n",
    "                  categories=2,K=K,K_inner=inner_K,device=torch.device('cpu'),optimizer='Adam')\n",
    "print(mean_acc,out_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cv and fcv\n",
    "For cv and fcv protocols, simply change the 'protocols' parameter. And 'K_inner' is no longer a necessity. We only present DGCNN model with cv and fcv protocol, other models can be applied in similar methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paras': {'lr': 0.01, 'hiddens': 20}, 'acc_mean': 0.6388888888888888, 'argmax_epoch': -1} [{'paras': {'lr': 0.01, 'hiddens': 10}, 'acc_mean': 0.6111111111111112, 'argmax_epoch': -1}, {'paras': {'lr': 0.001, 'hiddens': 10}, 'acc_mean': 0.5, 'argmax_epoch': -1}, {'paras': {'lr': 0.01, 'hiddens': 20}, 'acc_mean': 0.6388888888888888, 'argmax_epoch': -1}, {'paras': {'lr': 0.001, 'hiddens': 20}, 'acc_mean': 0.5, 'argmax_epoch': -1}]\n",
      "{'paras': {'lr': 0.01, 'hiddens': 10}, 'acc_mean': 0.6944444444444444, 'argmax_epoch': 1} [{'paras': {'lr': 0.01, 'hiddens': 10}, 'acc_mean': 0.6944444444444444, 'argmax_epoch': 1}, {'paras': {'lr': 0.001, 'hiddens': 10}, 'acc_mean': 0.6944444444444444, 'argmax_epoch': 1}, {'paras': {'lr': 0.01, 'hiddens': 20}, 'acc_mean': 0.6111111111111112, 'argmax_epoch': 3}, {'paras': {'lr': 0.001, 'hiddens': 20}, 'acc_mean': 0.5277777777777778, 'argmax_epoch': 1}]\n"
     ]
    }
   ],
   "source": [
    "# DGCNN under cv with intra_subject\n",
    "loader=data_split('intra_subject',data,label,subject_id_list)\n",
    "best_dict,out_acc_list=evaluation(model_DGCNN,loader,'cv',grid={\"lr\":[0.01,0.001],\"hiddens\":[10,20],'epoch':list(range(0,10))},\n",
    "                  categories=2,K=K,device=torch.device('cpu'),optimizer='Adam')\n",
    "print(best_dict,out_acc_list)\n",
    "\n",
    "# DGCNN under fcv with cross_subject\n",
    "loader=data_split('cross_subject',data,label,subject_id_list)\n",
    "best_dict,out_acc_list=evaluation(model_DGCNN,loader,'fcv',grid={\"lr\":[0.01,0.001],\"hiddens\":[10,20],'epoch':list(range(0,10))},\n",
    "                  categories=2,K=K,device=torch.device('cpu'),optimizer='Adam')\n",
    "print(best_dict,out_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data_FACED\n",
    "By the end of this notebook, we would like to show the usage of data_FACED. Please make sure the shape of the data is *(123, 720 or 840, 120)* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The path of FACED dataset does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# the data_path is just an example which in fact does not exist.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./src/my_FACED_dataset.mat\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m loader\u001b[39m=\u001b[39mdata_FACED(\u001b[39m'\u001b[39;49m\u001b[39mcross_subject\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m2\u001b[39;49m,data_path)\n",
      "File \u001b[0;32m~/GNN4EEG/ge/protocols.py:126\u001b[0m, in \u001b[0;36mdata_FACED\u001b[0;34m(protocol, categories, data_path)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe categories should be either 2 or 9.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(data_path) \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe path of FACED dataset does not exist.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[39mif\u001b[39;00m categories \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m categories \u001b[39m!=\u001b[39m \u001b[39m9\u001b[39m:\n\u001b[1;32m    129\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    130\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe label categories in FACED dataset should be either 2 or 9.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The path of FACED dataset does not exist."
     ]
    }
   ],
   "source": [
    "# the data_path is just an example which in fact does not exist.\n",
    "data_path='./src/my_FACED_dataset.mat'\n",
    "loader=data_FACED('cross_subject',2,data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FACED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
